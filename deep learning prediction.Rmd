---
title: "deep learning for euro dollar prediction"
author: "kuldeep singh bhati"
output: html_document
---
The aim of the project is to train deep neural network using DN SAE model and predict next bar up and down for euro/dollar. 
Input data for the project use
https://github.com/kuldeepldec/deep-machine-learning-on-euro-dollar1



## Libraries required for the project
```{r}
library(caret)
library(deepnet)
library(quantmod)
library(rminer)
library(TTR)
```
```{r}
set.seed(1)
```

###first read table euro/dollar 26 minute data data.
```{r}
price<-read.table("euro.txt",header=TRUE,sep=",")
price=subset(price[,3:6])

```
###Calculating median and range for the bar
```{r}
Med <- (price[,2] + price[,3])/2
CO <- price[, 4] - price[, 1]
```
###Add Med and CO prices to the matrix
```{r}
price <- cbind(price, Med, CO)
```
##Now producing indicators for input data,making an Input function

```{r}
Input<-function(p = 14){
  adx<-ADX(price, n = p)
  ar<-aroon(price[ ,c('High', 'Low')], n=p)[ ,'oscillator']
  cci<-CCI(price[ ,2:4], n = p)
  chv<-chaikinVolatility(price[ ,2:4], n = p)
  cmo<-CMO(price[ ,'Med'], n = p)
  macd<-MACD(price[ ,'Med'], 12, 26, 9)[ ,'macd']
  osma<-macd - MACD(price[ ,'Med'],12, 26, 9)[ ,'signal']
  rsi<-RSI(price[ ,'Med'], n = p)
  stoh<-stoch(price[ ,2:4],14, 3, 3)
  vol<-volatility(price[ ,1:4],n = p,calc="yang.zhang", N=96)
  xavg<-EMA(price[,4],n=p)
  trend<-price[,4]-xavg;
  atr5<-ATR(HLC(price),5)
  atr5<-atr5[,2]
  
  Input<-cbind(adx, ar, cci, chv, cmo, macd, osma, rsi, stoh,vol,xavg,trend,atr5)
  return(Input)
}
```



###to access In Function
```{r}
X<-Input()
```


##Making an out function for buy and sell signal, out signal take median price and calculate difference between two median prices.if value is postive assign 0 and if value is negative assign 0

```{r}
Out<-function(ch=0.0037){
  
  zz<-ZigZag(price[ ,'Med'], change = ch, percent = F, retrace = F, lastExtreme = T)
  n<-1:length(zz)
  # On the last bars substitute the undefined values for the last known ones
  for(i in n) { if(is.na(zz[i])) zz[i] = zz[i-1]}
  #Define the speed of ZigZag changes and move one bar forward
  dz<-c(diff(zz), NA)
  #If the speed >0 - signal = 0(Buy), if <0, signal = 1 (Sell) otherwise NA
  sig<-ifelse(dz>0, 0, ifelse(dz<0, 1, NA))
  return(sig)
}
```

```{r}
Y<-Out()
table(Y)
```
##clearing Na values using clearing function
```{r}
Clearing<-function(x, y){
  dt<-cbind(x,y);
  n<-ncol(dt)
  dt<-na.omit(dt)
  return(dt);  
}
```

```{r}
dt<-Clearing(X,Y)
```
##Balancing function check if 1 and 0 signal are equal or not, if they are not equal it insert 0 to columns of y with 1.
```{r}
Balancing<-function(DT){
  #Calculate a table with a number of classes
  cl<-table(DT[ ,ncol(DT)]);
  #If the divergence is less than 15%, return the initial matrix
  if(max(cl)/min(cl)<= 1.15) return(DT)
  #Otherwise level by the greater side
  DT<-if(max(cl)/min(cl)> 1.15){ 
         upSample(x = DT[ ,-ncol(DT)],y = as.factor(DT[ , ncol(DT)]), yname = "Y")
        }
  #Convert ? (factor) into a number
  DT$Y<-as.numeric(DT$Y)
  #Recode ? from 1,2 into 0,1
  DT$Y<-ifelse(DT$Y == 1, 0, 1)
  #Convert dataframe to matrix
  DT<-as.matrix(DT)
  return(DT);
}
```
###Seperating value of x and Y
```{r}
dt.b<-Balancing(dt)
x<-dt.b[ ,-ncol(dt.b)]
y<-dt.b[ , ncol(dt.b)]
```

##dividing our data into train and test data set
```{r}
t<-holdout(y, ratio = 8/10, mode = "random")
```

The t object is a list containing indices of the training (t$tr) and the test (t$ts) data set. The received sets are stratified.

####preProcessing data as Neural networks can receive the input variables in the range (-1; 1) or (0, 1). Normalize the input variables into the range of [-1, 1]. 
```{r}
spSign<-preProcess(x[t$tr, ], method = "spatialSign")
x.tr<-predict(spSign, x[t$tr, ])
x.ts<-predict(spSign, x[t$ts, ])
```

##We are going to build and train the DN SAE model. 
```{r}
SAE<-sae.dnn.train(x= x.tr, y= y[t$tr], hidden=c(100,100,100), activationfun = "tanh", learningrate = 0.6, momentum = 0.5, learningrate_scale = 1.0, output = "sigm", sae_output = "linear", numepochs = 10, batchsize = 100, hidden_dropout = 0, visible_dropout = 0)
```

As we can see, it happens in two stages. At first autoencoder gets trained layer by layer and then the neural network.

##Let us evaluate forecasts on the test set of predictors.
```{r}
pr.sae<-nn.predict(SAE, x.ts)
```

###Convert into the levels 0,1 and calculate measurements
```{r}
pr<-ifelse(pr.sae>mean(pr.sae), 1, 0)
confusionMatrix(y[t$ts], pr)
```
##Predicting profit on last 1000 bars
```{r}
new.x<-predict(spSign,tail(dt[ ,-ncol(dt)], 1000))
pr.sae1<-nn.predict(SAE, new.x)

pr.sig<-ifelse(pr.sae1>mean(pr.sae1), -1, 1)
table(pr.sig)

new.y<-ifelse(tail(dt[, ncol(dt)], 1000) == 0, 1, -1)
table(new.y)

cm1<-confusionMatrix(new.y, pr.sig)
cm1

```
##Test the profit for the last 1000 bars using our predicted signals and get the balance curve:
```{r}
bal<-cumsum(tail(price[ , 'CO'], 1000) * pr.sig)
plot(bal, t = "l")
abline(h = 0)
```
##Now compare with the balance that would have been obtained from the ideal signals of ZZ. The red line is the balance by the neural network signals:
```{r}
bal.zz<-cumsum(tail(price[ , 'CO'], 1000) * new.y)
plot(bal.zz,  t = "l")
lines(bal,  col = 2)
```








